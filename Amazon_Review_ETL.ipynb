import os
# Find the latest version of spark 3.0 from http://www.apache.org/dist/spark/ and enter as the spark version
# For example:
# spark_version = 'spark-3.0.3'

spark_version = 'spark-3.0.3'
os.environ['SPARK_VERSION']=spark_version

# Install Spark and Java

!apt-get update
!apt-get install openjdk-11-jdk-headless -qq > /dev/null
!wget -q http://www.apache.org/dist/spark/$SPARK_VERSION/$SPARK_VERSION-bin-hadoop2.7.tgz
!tar xf $SPARK_VERSION-bin-hadoop2.7.tgz
!pip install -q findspark

# Set Environment Variables

import os
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-11-openjdk-amd64"
os.environ["SPARK_HOME"] = f"/content/{spark_version}-bin-hadoop2.7"

# Start a SparkSession

import findspark
findspark.init()

# Download the Postgres driver that will allow Spark to interact with Postgres.

!wget https://jdbc.postgresql.org/download/postgresql-42.2.16.jar

from pyspark.sql import SparkSession
spark = SparkSession.builder.appName("M16-Amazon-Challenge").config("spark.driver.extraClassPath","/content/postgresql-42.2.16.jar").getOrCreate()

from pyspark import SparkFiles
url = "https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Video_Games_v1_00.tsv.gz"
spark.sparkContext.addFile(url)
df = spark.read.option("encoding", "UTF-8").csv(SparkFiles.get("amazon_reviews_us_Video_Games_v1_00.tsv.gz"), sep="\t", header=True, inferSchema=True)
df.show()


df.printSchema()

print((df.count(), len(df.columns)))

from pyspark.sql.functions import to_date

# Read in the Review dataset as a DataFrame

drop_na_df = df.dropna()
drop_na_df.show()

print((drop_na_df.count(), len(drop_na_df.columns)))

# Create the customers_table DataFrame

customers_df = drop_na_df.groupby("customer_id").agg({"customer_id":"count"}).withColumnRenamed("count(customer_id)", "customer_count")
customers_df.show()

# Create the products_table DataFrame and drop duplicates. 

products_df = drop_na_df.select(["product_id", "product_title"]).drop_duplicates()
products_df.show()

# Create the review_id_table DataFrame. 
# Convert the 'review_date' column to a date datatype with to_date("review_date", 'yyyy-MM-dd').alias("review_date")

review_id_df = drop_na_df.select(["review_id","customer_id","product_id","product_parent", to_date("review_date", 'yyyy-MM-dd').alias("review_date")])
review_id_df.show()

review_id_df.printSchema()

# Create the vine_table. DataFrame

vine_df = drop_na_df.select(["review_id","star_rating","helpful_votes","total_votes","vine","verified_purchase"])
vine_df.show()

# adding the stuff from module to ask for password
# from getpass import getpass
# password = getpass('Enter database password')

# Configure settings for RDS

mode = "append"
jdbc_url="jdbc:postgresql://____:5432/module_challenge_sms"
config = {"user":"postgres", 
          "password": "___", 
          "driver":"org.postgresql.Driver"}

# Write review_id_df to table in RDS

review_id_df.write.jdbc(url=jdbc_url, table='review_id_table', mode=mode, properties=config)

# Write products_df to table in RDS
# about 3 min

products_df.write.jdbc(url=jdbc_url, table='products_table', mode=mode, properties=config)

# Write customers_df to table in RDS
# 5 min 14 s

customers_df.write.jdbc(url=jdbc_url, table='customers_table', mode=mode, properties=config)

# Write vine_df to table in RDS
# 11 minutes

vine_df.write.jdbc(url=jdbc_url, table='vine_table', mode=mode, properties=config)